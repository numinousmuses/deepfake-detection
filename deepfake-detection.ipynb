{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/joshuaokolo/deepfake-detection?scriptVersionId=104093178\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Score Metric","metadata":{}},{"cell_type":"code","source":"# SKLearn Implemention\nfrom sklearn.metrics import log_loss\nlog_loss([\"REAL\", \"FAKE\", \"FAKE\", \"REAL\"],\n         [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport cv2\nplt.style.use('ggplot')\nfrom IPython.display import Video\nfrom IPython.display import HTML","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -GFlash ../input/deepfake-detection-challenge","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!du -sh ../input/deepfake-detection-challenge/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sample_metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\ntrain_sample_metadata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sample_metadata.groupby('label')['label'].count().plot(figsize=(15, 5), kind='bar', title='Distribution of Labels in the Training Set')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Detect faces in video","metadata":{}},{"cell_type":"code","source":"import cv2 as cv\nimport os\nimport matplotlib.pylab as plt\ntrain_dir = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/'\nfig, ax = plt.subplots(1,1, figsize=(15, 15))\ntrain_video_files = [train_dir + x for x in os.listdir(train_dir)]\n# video_file = train_video_files[30]\nvideo_file = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/akxoopqjqz.mp4'\ncap = cv.VideoCapture(video_file)\nsuccess, image = cap.read()\nimage = cv.cvtColor(image, cv.COLOR_BGR2RGB)\ncap.release()   \nax.imshow(image)\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\nax.title.set_text(f\"FRAME 0: {video_file.split('/')[-1]}\")\nplt.grid(False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install face_recognition","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import face_recognition\nface_locations = face_recognition.face_locations(image)\n\n# https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py\nfrom PIL import Image\n\nprint(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n\nfor face_location in face_locations:\n\n    # Print the location of each face in this image\n    top, right, bottom, left = face_location\n    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n\n    # You can access the actual face itself like this:\n    face_image = image[top:bottom, left:right]\n    fig, ax = plt.subplots(1,1, figsize=(5, 5))\n    plt.grid(False)\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_visible(False)\n    ax.imshow(face_image)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"face_landmarks_list = face_recognition.face_landmarks(image)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py\n# face_landmarks_list\nfrom PIL import Image, ImageDraw\npil_image = Image.fromarray(image)\nd = ImageDraw.Draw(pil_image)\n\nfor face_landmarks in face_landmarks_list:\n\n    # Print the location of each facial feature in this image\n    for facial_feature in face_landmarks.keys():\n        print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n\n    # Let's trace out each facial feature in the image with a line!\n    for facial_feature in face_landmarks.keys():\n        d.line(face_landmarks[facial_feature], width=3)\n\n# Show the picture\ndisplay(pil_image)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(19, 2, figsize=(15, 80))\naxs = np.array(axs)\naxs = axs.reshape(-1)\ni = 0\nfor fn in train_sample_metadata.index[:23]:\n    label = train_sample_metadata.loc[fn]['label']\n    orig = train_sample_metadata.loc[fn]['label']\n    video_file = f'/kaggle/input/deepfake-detection-challenge/train_sample_videos/{fn}'\n    ax = axs[i]\n    cap = cv.VideoCapture(video_file)\n    success, image = cap.read()\n    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n    face_locations = face_recognition.face_locations(image)\n    if len(face_locations) > 0:\n        # Print first face\n        face_location = face_locations[0]\n        top, right, bottom, left = face_location\n        face_image = image[top:bottom, left:right]\n        ax.imshow(face_image)\n        ax.grid(False)\n        ax.title.set_text(f'{fn} - {label}')\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        # Find landmarks\n        face_landmarks_list = face_recognition.face_landmarks(face_image)\n        face_landmarks = face_landmarks_list[0]\n        pil_image = Image.fromarray(face_image)\n        d = ImageDraw.Draw(pil_image)\n        for facial_feature in face_landmarks.keys():\n            d.line(face_landmarks[facial_feature], width=2)\n        landmark_face_array = np.array(pil_image)\n        ax2 = axs[i+1]\n        ax2.imshow(landmark_face_array)\n        ax2.grid(False)\n        ax2.title.set_text(f'{fn} - {label}')\n        ax2.xaxis.set_visible(False)\n        ax2.yaxis.set_visible(False)\n        i += 2\nplt.grid(False)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(19, 2, figsize=(10, 80))\naxs = np.array(axs)\naxs = axs.reshape(-1)\ni = 0\npad = 60\nfor fn in train_sample_metadata.index[23:44]:\n    label = train_sample_metadata.loc[fn]['label']\n    orig = train_sample_metadata.loc[fn]['label']\n    video_file = f'/kaggle/input/deepfake-detection-challenge/train_sample_videos/{fn}'\n    ax = axs[i]\n    cap = cv.VideoCapture(video_file)\n    success, image = cap.read()\n    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n    face_locations = face_recognition.face_locations(image)\n    if len(face_locations) > 0:\n        # Print first face\n        face_location = face_locations[0]\n        top, right, bottom, left = face_location\n        face_image = image[top-pad:bottom+pad, left-pad:right+pad]\n        ax.imshow(face_image)\n        ax.grid(False)\n        ax.title.set_text(f'{fn} - {label}')\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        # Find landmarks\n        face_landmarks_list = face_recognition.face_landmarks(face_image)\n        try:\n            face_landmarks = face_landmarks_list[0]\n            pil_image = Image.fromarray(face_image)\n            d = ImageDraw.Draw(pil_image)\n            for facial_feature in face_landmarks.keys():\n                d.line(face_landmarks[facial_feature], width=2, fill='white')\n            landmark_face_array = np.array(pil_image)\n            ax2 = axs[i+1]\n            ax2.imshow(landmark_face_array)\n            ax2.grid(False)\n            ax2.title.set_text(f'{fn} - {label}')\n            ax2.xaxis.set_visible(False)\n            ax2.yaxis.set_visible(False)\n            i += 2\n        except:\n            pass\nplt.grid(False)\nplt.tight_layout()\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"video_file = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/akxoopqjqz.mp4'\n\ncap = cv2.VideoCapture(video_file)\n\nframes = []\nwhile(cap.isOpened()):\n    ret, frame = cap.read()\n    if ret==True:\n        frames.append(frame)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    else:\n        break\ncap.release()\n\nprint('The number of frames saved: ', len(frames))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 3, figsize=(15, 10))\naxes = np.array(axes)\naxes = axes.reshape(-1)\n\nax_ix = 0\nfor i in [0, 25, 50, 75, 100, 125, 150, 175, 250]:\n    frame = frames[i]\n    #fig, ax = plt.subplots(1,1, figsize=(5, 5))\n    image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n    axes[ax_ix].imshow(image)\n    axes[ax_ix].xaxis.set_visible(False)\n    axes[ax_ix].yaxis.set_visible(False)\n    axes[ax_ix].set_title(f'Frame {i}')\n    ax_ix += 1\nplt.grid(False)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 3, figsize=(15, 15))\naxes = np.array(axes)\naxes = axes.reshape(-1)\nax_ix = 0\npadding = 40\nfor i in [0, 25, 50, 75, 100, 125, 150, 175, 250, 275]:\n    frame = frames[i]\n    #fig, ax = plt.subplots(1,1, figsize=(5, 5))\n    face_locations = face_recognition.face_locations(frame)\n    if len(face_locations) == 0:\n        print(f'Could not find face in frame {i}')\n        continue\n    top, right, bottom, left = face_locations[0]\n    frame_face = frame[top-padding:bottom+padding, left-padding:right+padding]\n    image = cv.cvtColor(frame_face, cv.COLOR_BGR2RGB)\n    axes[ax_ix].imshow(image)\n    axes[ax_ix].xaxis.set_visible(False)\n    axes[ax_ix].yaxis.set_visible(False)\n    axes[ax_ix].set_title(f'Frame {i}')\n    ax_ix += 1\nplt.grid(False)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 3, figsize=(15, 15))\naxes = np.array(axes)\naxes = axes.reshape(-1)\nax_ix = 0\npadding = 40\nfor i in [0, 25, 50, 75, 100, 125, 150, 175, 250, 275]:\n    frame = frames[i]\n    #fig, ax = plt.subplots(1,1, figsize=(5, 5))\n    face_locations = face_recognition.face_locations(frame)\n    if len(face_locations) == 0:\n        print(f'Count find face in frame {i}')\n        continue\n    top, right, bottom, left = face_locations[0]\n    frame_face = frame[top-padding:bottom+padding, left-padding:right+padding]\n    face_landmarks_list = face_recognition.face_landmarks(frame_face)\n    if len(face_landmarks_list) == 0:\n        print(f'Could not identify face landmarks for frame {i}')\n        continue\n    face_landmarks = face_landmarks_list[0]\n    pil_image = Image.fromarray(frame_face)\n    d = ImageDraw.Draw(pil_image)\n    for facial_feature in face_landmarks.keys():\n        d.line(face_landmarks[facial_feature], width=3, fill='white')\n    landmark_face_array = np.array(pil_image)\n    image = cv.cvtColor(landmark_face_array, cv.COLOR_BGR2RGB)\n    axes[ax_ix].imshow(image)\n    axes[ax_ix].grid(False)\n    axes[ax_ix].set_title(f'FAKE example - Frame {i}')\n    axes[ax_ix].xaxis.set_visible(False)\n    axes[ax_ix].yaxis.set_visible(False)\n    ax_ix += 1\nplt.grid(False)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inspired by [Rob Mulla](https://www.kaggle.com/code/robikscube/kaggle-deepfake-detection-introduction/notebook)\n\nOther references:\n\nhttps://github.com/ageitgey/face_recognition","metadata":{}}]}